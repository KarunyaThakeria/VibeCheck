{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4f71095-4a96-409d-aa3a-c7ed7809464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LogisticRegression is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76f89e3f-8091-47be-967c-03c3d65069d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from statistics import mean\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1d972d4-f65d-44dd-9813-9f37a7f51079",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data\n",
    "train = pd.read_csv(\"SE/train(T).csv\")\n",
    "test = pd.read_csv(\"SE/test(T).csv\")\n",
    "test_y = pd.read_csv(\"SE/test_labels(T).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5516b121-897f-4b6d-87da-c23e95a2e18f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of clean comment\n",
    "train.comment_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4a5c9a6-0a37-4136-a2d9-23c241688fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hey... what is it..\\n@ | talk .\\nWhat is it... an exclusive group of some WP TALIBANS...who are good at destroying, self-appointed purist who GANG UP any one who asks them questions abt their ANTI-SOCIAL and DESTRUCTIVE (non)-contribution at WP?\\n\\nAsk Sityush to clean up his behavior than issue me nonsensical warnings...'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of toxic comment\n",
    "train[train.toxic == 1].iloc[1, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005696c5-abe5-41ad-bd56-e863e131ccda",
   "metadata": {},
   "source": [
    "#Feature-engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03a90446-f0a3-4e47-9d5d-84f914ec4e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75eb7265-a7d1-4fe4-a239-941efdbfef69",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = [\"toxic\", \"severe_toxic\", \"obscene\",\"threat\", \"insult\", \"identity_hate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7baf8436-fc21-49c4-8af4-43ca5788700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Tokenize text and return a non-unique list of tokenized words found in the text. \n",
    "    Normalize to lowercase, strip punctuation, remove stop words, filter non-ascii characters.\n",
    "    Lemmatize the words and lastly drop words of length < 3.\n",
    "    '''\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n",
    "    nopunct = regex.sub(\" \", text)\n",
    "    words = nopunct.split(' ')\n",
    "    # remove any non ascii\n",
    "    words = [word.encode('ascii', 'ignore').decode('ascii') for word in words]\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    words = [lmtzr.lemmatize(w) for w in words]\n",
    "    words = [w for w in words if len(w) > 2]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "957da771-5e60-4bbf-b819-ac9908e67064",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = TfidfVectorizer(ngram_range=(1, 1), analyzer='word',\n",
    "                         tokenizer=tokenize, stop_words='english',\n",
    "                         strip_accents='unicode', use_idf=True, min_df=10)\n",
    "X_train = vector.fit_transform(train['comment_text'])\n",
    "X_test = vector.transform(test['comment_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56277101-b7b0-4afb-98e4-c900569dc6cb",
   "metadata": {},
   "source": [
    "#Modeling and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cd884203-20fd-4f39-a23b-7d2ddab9310c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating classifiers with default parameters initially.\n",
    "clf2 = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "151bff52-7354-4b53-99a0-b0abeb3b9601",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate though each label and return the cross validation F1 and Recall score \n",
    "def cross_validation_score(classifier, X_train, y_train):\n",
    "    methods = []\n",
    "    name = classifier.__class__.__name__.split('.')[-1]\n",
    "\n",
    "    for label in test_labels:\n",
    "        recall = cross_val_score(\n",
    "            classifier, X_train, y_train[label], cv=10, scoring='recall')\n",
    "        f1 = cross_val_score(classifier, X_train,\n",
    "                             y_train[label], cv=10, scoring='f1')\n",
    "        methods.append([name, label, recall.mean(), f1.mean()])\n",
    "\n",
    "    return methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d807e703-91ac-44ff-94e6-bf5fe21e332c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Calculating the cross validation F1 and Recall score for our 3 baseline models.\n",
    "\n",
    "methods2_cv = pd.DataFrame(cross_validation_score(clf2, X_train, train))\n",
    "print(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9725d1bd-d4b4-4f34-83f3-5484665552e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Label</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.612528</td>\n",
       "      <td>0.732122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>severe_toxic</td>\n",
       "      <td>0.255185</td>\n",
       "      <td>0.351210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>obscene</td>\n",
       "      <td>0.638658</td>\n",
       "      <td>0.748026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>threat</td>\n",
       "      <td>0.131605</td>\n",
       "      <td>0.218787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>insult</td>\n",
       "      <td>0.523292</td>\n",
       "      <td>0.636648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.212837</td>\n",
       "      <td>0.323868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Model          Label    Recall        F1\n",
       "0  LogisticRegression          toxic  0.612528  0.732122\n",
       "1  LogisticRegression   severe_toxic  0.255185  0.351210\n",
       "2  LogisticRegression        obscene  0.638658  0.748026\n",
       "3  LogisticRegression         threat  0.131605  0.218787\n",
       "4  LogisticRegression         insult  0.523292  0.636648\n",
       "5  LogisticRegression  identity_hate  0.212837  0.323868"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a dataframe to show summary of results.\n",
    "methods2_cv.columns = ['Model', 'Label', 'Recall', 'F1']\n",
    "meth_cv = methods2_cv.reset_index()\n",
    "meth_cv[['Model', 'Label', 'Recall', 'F1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d909534b-e951-4ec6-839d-16e3047bead6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on the cross validation above, we noticed that overall, the linear SVC model and Logistic Regression model perform better. \n",
    "#As a baseline model, Multinomial Naive Bayes does not perform well, especially for the threat label and identity_hate label because these two \n",
    "#labels have the least number of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e3d607aa-6f0d-4514-a310-07a2660fe9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we want to see how these three models perform on the actual prediction - the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "66d89650-2d59-4ca3-92ee-8d6c3aaac792",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Hamming-loss, F1, Recall for each label on test dataset.\n",
    "\n",
    "def score(classifier, X_train, y_train, X_test, y_test):\n",
    "    methods = []\n",
    "    hloss = []\n",
    "    name = classifier.__class__.__name__.split('.')[-1]\n",
    "    predict_df = pd.DataFrame()\n",
    "    predict_df['id'] = test_y['id']\n",
    "\n",
    "    for label in test_labels:\n",
    "        classifier.fit(X_train, y_train[label])\n",
    "        predicted = classifier.predict(X_test)\n",
    "\n",
    "        predict_df[label] = predicted\n",
    "\n",
    "        recall = recall_score(y_test[y_test[label] != -1][label],\n",
    "                              predicted[y_test[label] != -1],\n",
    "                              average=\"weighted\")\n",
    "        f1 = f1_score(y_test[y_test[label] != -1][label],\n",
    "                      predicted[y_test[label] != -1],\n",
    "                      average=\"weighted\")\n",
    "\n",
    "        conf_mat = confusion_matrix(y_test[y_test[label] != -1][label],\n",
    "                                    predicted[y_test[label] != -1])\n",
    "\n",
    "        methods.append([name, label, recall, f1, conf_mat])\n",
    "\n",
    "    hamming_loss_score = hamming_loss(test_y[test_y['toxic'] != -1].iloc[:, 1:7],\n",
    "                                      predict_df[test_y['toxic'] != -1].iloc[:, 1:7])\n",
    "    hloss.append([name, hamming_loss_score])\n",
    "\n",
    "    return hloss, methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "03d4aafc-6267-427c-98da-31abb29bf56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the Hamming-loss F1 and Recall score for our 3 baseline models.\n",
    "h2, methods2 = score(clf2, X_train, train, X_test, test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "15446b65-9336-46cb-8b23-d63993273852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Label</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>toxic</td>\n",
       "      <td>0.934321</td>\n",
       "      <td>0.936142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>severe_toxic</td>\n",
       "      <td>0.993154</td>\n",
       "      <td>0.992851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>obscene</td>\n",
       "      <td>0.966066</td>\n",
       "      <td>0.964416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>threat</td>\n",
       "      <td>0.996374</td>\n",
       "      <td>0.995736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>insult</td>\n",
       "      <td>0.963644</td>\n",
       "      <td>0.961003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.990419</td>\n",
       "      <td>0.988397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Model          Label    Recall        F1\n",
       "0  LogisticRegression          toxic  0.934321  0.936142\n",
       "1  LogisticRegression   severe_toxic  0.993154  0.992851\n",
       "2  LogisticRegression        obscene  0.966066  0.964416\n",
       "3  LogisticRegression         threat  0.996374  0.995736\n",
       "4  LogisticRegression         insult  0.963644  0.961003\n",
       "5  LogisticRegression  identity_hate  0.990419  0.988397"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a dataframe to show summary of results.\n",
    "methods2 = pd.DataFrame(methods2)\n",
    "methods2.columns = ['Model', 'Label', 'Recall', 'F1', 'Confusion_Matrix']\n",
    "meth = methods2.reset_index()\n",
    "meth[['Model', 'Label', 'Recall', 'F1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d9fb67-ba8f-4812-a026-f25e1663c311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9461f03-2673-42ec-a72f-eeb0260ca5b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
