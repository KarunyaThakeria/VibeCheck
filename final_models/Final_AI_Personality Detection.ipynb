{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f5794c0-cda0-4c46-b0a7-43d3d1c66754",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forest is used for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a42c4d4-c576-488c-bf1e-2597a4a20ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb0613e5-df91-44de-8f4d-e680b67a42bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d92e1600-e5ae-4435-939d-bbfac047f786",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=PorterStemmer()\n",
    "df = pd.read_csv(r\"C:\\Users\\karun\\Downloads\\Data Set and Images used\\Data Set and Images used\\Personality(P).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c3f58d7-6196-4e08-81be-745c142b28df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   type                                              posts\n",
      "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
      "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
      "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
      "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
      "4  ENTJ  'You're fired.|||That's another silly misconce...\n",
      "5  INTJ  '18/37 @.@|||Science  is not perfect. No scien...\n",
      "6  INFJ  'No, I can't draw on my own nails (haha). Thos...\n",
      "7  INTJ  'I tend to build up a collection of things on ...\n",
      "8  INFJ  I'm not sure, that's a good question. The dist...\n",
      "9  INTP  'https://www.youtube.com/watch?v=w8-egj0y8Qs||...\n"
     ]
    }
   ],
   "source": [
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a027f8d0-08f7-459d-883a-cc313c3e60e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8675 entries, 0 to 8674\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   type    8675 non-null   object\n",
      " 1   posts   8675 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 135.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff966e72-a61d-4dbd-a083-94c5a3c01f73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2594b1d-d850-4772-ba8b-140172c2310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e3b6ae48-577b-4de7-b223-b41a13bf3f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\|'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\|'\n",
      "C:\\Users\\karun\\AppData\\Local\\Temp\\ipykernel_15492\\708078908.py:1: SyntaxWarning: invalid escape sequence '\\|'\n",
      "  \"\"\"def cleanText(text):\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def cleanText(text):\n",
    "    text = re.sub(r'https?://(?:www\\.)?(?:youtube|youtu|youtube-nocookie)\\.com(?:[^\\s]+)', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text) \n",
    "    text = text.replace(\"|||\", \" \")\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text, flags=re.MULTILINE)\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    text = ' '.join(tokens)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5ec2a37-504e-45ed-9169-ac4e56807649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>I-E</th>\n",
       "      <th>N-S</th>\n",
       "      <th>T-F</th>\n",
       "      <th>J-P</th>\n",
       "      <th>http_per_comment</th>\n",
       "      <th>music_per_comment</th>\n",
       "      <th>question_per_comment</th>\n",
       "      <th>img_per_comment</th>\n",
       "      <th>excl_per_comment</th>\n",
       "      <th>ellipsis_per_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>' intj moments sportscenter top ten plays pran...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>' 'm finding lack posts alarming . sex boring ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'good one _____ course , say know ; 's blessin...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'dear intp , enjoyed conversation day . esoter...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'you 're fired . 's another silly misconceptio...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts  I-E  N-S  T-F  \\\n",
       "0  INFJ  ' intj moments sportscenter top ten plays pran...    0    0    1   \n",
       "1  ENTP  ' 'm finding lack posts alarming . sex boring ...    1    0    0   \n",
       "2  INTP  'good one _____ course , say know ; 's blessin...    0    0    0   \n",
       "3  INTJ  'dear intp , enjoyed conversation day . esoter...    0    0    0   \n",
       "4  ENTJ  'you 're fired . 's another silly misconceptio...    1    0    0   \n",
       "\n",
       "   J-P  http_per_comment  music_per_comment  question_per_comment  \\\n",
       "0    0               0.0               0.02                  0.04   \n",
       "1    1               0.0               0.00                  0.08   \n",
       "2    1               0.0               0.00                  0.18   \n",
       "3    0               0.0               0.02                  0.18   \n",
       "4    0               0.0               0.02                  0.16   \n",
       "\n",
       "   img_per_comment  excl_per_comment  ellipsis_per_comment  \n",
       "0              0.0              0.06                  0.30  \n",
       "1              0.0              0.00                  0.38  \n",
       "2              0.0              0.08                  0.26  \n",
       "3              0.0              0.06                  0.52  \n",
       "4              0.0              0.02                  0.42  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['posts'] = df['posts'].apply(cleanText)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "082906ce-e160-43b1-9ba0-da6f717e206c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividing the 16 personalities into 4 major groups for ease\n",
    "map1 = {\"I\": 0, \"E\": 1}\n",
    "map2 = {\"N\": 0, \"S\": 1}\n",
    "map3 = {\"T\": 0, \"F\": 1}\n",
    "map4 = {\"J\": 0, \"P\": 1}\n",
    "df['I-E'] = df['type'].astype(str).str[0]\n",
    "df['I-E'] = df['I-E'].map(map1)\n",
    "df['N-S'] = df['type'].astype(str).str[1]\n",
    "df['N-S'] = df['N-S'].map(map2)\n",
    "df['T-F'] = df['type'].astype(str).str[2]\n",
    "df['T-F'] = df['T-F'].map(map3)\n",
    "df['J-P'] = df['type'].astype(str).str[3]\n",
    "df['J-P'] = df['J-P'].map(map4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1694fab9-4ba2-44c4-a4de-8f965684b17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['http_per_comment'] = df['posts'].apply(lambda x: x.count('http')/50)\n",
    "df['music_per_comment'] = df['posts'].apply(lambda x: x.count('music')/50)\n",
    "df['question_per_comment'] = df['posts'].apply(lambda x: x.count('?')/50)\n",
    "df['img_per_comment'] = df['posts'].apply(lambda x: x.count('jpg')/50)\n",
    "df['excl_per_comment'] = df['posts'].apply(lambda x: x.count('!')/50)\n",
    "df['ellipsis_per_comment'] = df['posts'].apply(lambda x: x.count('...')/50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb18db85-1e87-49dc-b5b6-598fce97ddd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building ML on 'type' column \n",
    "X = df.drop(['type','posts','I-E','N-S','T-F','J-P'], axis=1).values\n",
    "y = df['type'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d87b5a6a-0d12-491c-931f-396bfc33ed9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8675,)\n",
      "(8675, 6)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65dc81fd-6882-42d1-b3f1-c08257912fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ecf82c3-525a-4f25-aa04-8f08600524f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outcome shape (8675,)\n",
      "input shape for machine learning data (8675, 9)\n"
     ]
    }
   ],
   "source": [
    "#Ml on judging/perceiving column\n",
    "X5 = df.drop(['type','posts','J-P'], axis=1).values\n",
    "y5 = df['J-P'].values\n",
    "\n",
    "print(\"outcome shape\",y5.shape)\n",
    "print(\"input shape for machine learning data\",X5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6db00bd2-803c-438b-94ba-cb2125fc5734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Predictions Model 98.22 %\n"
     ]
    }
   ],
   "source": [
    "X5_train,X5_test,y5_train,y5_test=train_test_split(X5,y5,test_size = 0.1, random_state=5)\n",
    "\n",
    "random_forestt = RandomForestClassifier(n_estimators=100)\n",
    "random_forestt.fit(X5_train, y5_train)\n",
    "\n",
    "Y_prediction = random_forestt.predict(X5_test)\n",
    "\n",
    "random_forestt.score(X5_train, y5_train)\n",
    "acc_random_forestt = round(random_forestt.score(X5_train, y5_train) * 100, 2)\n",
    "print(\"Random Forest Predictions Model\",round(acc_random_forestt,2,), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d69cf1-ebba-4677-82d1-7c4e5baff7ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
